{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習済みパラメータを読み込んで推論する\n",
    "パラメータ読み込みには、使用モデルと全てのハイパーパラメータが一致している必要がある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ディレクトリ構成：  \n",
    "```\n",
    "./\n",
    "├dataset/\n",
    "│   └(読み込むCSVファイル)\n",
    "├models/\n",
    "│   └(モデルパラメータキャッシュファイル)\n",
    "├notebooks/\n",
    "│   └inference_seq2seq.ipynb(このファイル)\n",
    "├results/\n",
    "│   └(実行結果CSVと学習曲線グラフ画像)\n",
    "└src/\n",
    "   ├common/\n",
    "   │ └(共用クラス、関数)\n",
    "   ├data/\n",
    "   │ └(データ読み書き用コード)\n",
    "   ├attention_layer.py\n",
    "   ├attention_seq2seq.py\n",
    "   ├peeky_seq2seq.py\n",
    "   └seq2seq.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from data.sequence import TextSequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from seq2seq import Seq2seq\n",
    "from peeky_seq2seq import PeekySeq2seq\n",
    "from attention_seq2seq import AttentionSeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlibのインライン表示\n",
    "%matplotlib inline\n",
    "# モジュールの更新時に自動で再読み込み\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディレクトリパス\n",
    "dataset_dir =Path('../dataset')\n",
    "result_dir = Path('../results')\n",
    "model_dir = Path('../models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルパスの設定\n",
    "vocab_csv = [dataset_dir /  \"interpretation_train43_and_test40_rev3.csv\", dataset_dir / \"interpretation_excel_concat_rev2.csv\"]\n",
    "target_csv = dataset_dir / \"interpretation_train43_and_test40_rev3.csv\"\n",
    "# target_csv = dataset_dir / \"interpretation_excel_concat_rev2.csv\"\n",
    "dataset_name = target_csv.stem\n",
    "encoding = \"utf-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータファイルパス(ファイル名にハイパーパラメータを記載しているので、確認しましょう)\n",
    "pickle_path = str(model_dir / \"interpretation_train43_and_test40_rev2_PeekySeq2seq_V456_D128_H128_190207_1754.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全てのデータセットを読み込み、Vocabularyを作成する\n",
    "seq = TextSequence()\n",
    "for path in vocab_csv:\n",
    "    seq.read_csv(path)\n",
    "char_to_id, id_to_char = seq.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータ\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 128\n",
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "max_epoch = 100\n",
    "max_grad = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論対象のデータセットを読み込み\n",
    "x_target, t_target = seq.read_csv(target_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル選択\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_params(file_path=pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "start_id = seq.start_id\n",
    "sample_size = seq.sample_size\n",
    "guess_target = model.generate(x_target, start_id, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存ファイルのファイル名生成とディレクトリ作成\n",
    "modelname = model.__class__.__name__\n",
    "timestamp = datetime.now().strftime(\"_%y%m%d_%H%M\")\n",
    "save_dir = result_dir / (dataset_name + timestamp)\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果をCSVに保存\n",
    "result_csv = save_dir /  (\"result_\" + dataset_name + \"_\" + modelname + \".csv\")\n",
    "df_result = seq.result_to_csv(result_csv, x_target, t_target, guess_target, encoding=encoding)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231.992px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
